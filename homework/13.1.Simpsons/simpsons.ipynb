{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simpsons.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+Z0T9bR8k/H7NV9P7h2y/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qhDYYyZRZlSp","colab_type":"text"},"source":["## **Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sUyBTi6NZ0rV","colab_type":"text"},"source":["Основным заданием будет обучить классификатор на основе сверточных сетей, чтобы научиться отличать всех жителей Спрингфилда. мин пороговый скор 0,95. \n","\n","сабмит грузится на каггл\n","https://www.kaggle.com/c/journey-springfield/overview"]},{"cell_type":"code","metadata":{"id":"2p8mIIFzZwJr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404480483,"user_tz":-180,"elapsed":1305,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import pickle\n","from skimage import io\n","from tqdm import tqdm, tqdm_notebook\n","from PIL import Image\n","from pathlib import Path\n","from multiprocessing.pool import ThreadPool\n","from sklearn.preprocessing import LabelEncoder\n","from matplotlib import colors, pyplot as plt\n","%matplotlib inline\n","\n","# в sklearn не все гладко, чтобы в colab удобно выводить картинки \n","# мы будем игнорировать warnings\n","import warnings\n","warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n","\n","plt.ion()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uLviccKbXzK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597404480488,"user_tz":-180,"elapsed":534,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}},"outputId":"b69cb125-d28d-4e18-cc90-116a2195aefb"},"source":["train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lYLJOoJ9duG7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1597400120337,"user_tz":-180,"elapsed":21293,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}},"outputId":"39292026-30fd-42c3-a9d1-eb7213370e5c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h8AEmcLcduP1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597400243248,"user_tz":-180,"elapsed":12450,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["!unzip -q '/content/gdrive/My Drive/dl_stepik/homework_11_simpsons/journey-springfield.zip' "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"P1-eP7VhduNX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597404486484,"user_tz":-180,"elapsed":2492,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}},"outputId":"5f8a045e-0aea-4bab-f9fd-0638004a3873"},"source":["!ls train"],"execution_count":3,"outputs":[{"output_type":"stream","text":["simpsons_dataset\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X3Gwzc4WduLH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"ok","timestamp":1597404488249,"user_tz":-180,"elapsed":1898,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}},"outputId":"93a003b5-11ca-449d-9f37-4d584d6f3152"},"source":["!nvidia-smi\n","#import torch\n","torch.cuda.is_available()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Fri Aug 14 11:28:08 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   70C    P8    11W /  70W |     10MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"TJt37RZteonY","colab_type":"text"},"source":["В тесте будет 990 картнок, для которых вам будет необходимо предсказать класс."]},{"cell_type":"code","metadata":{"id":"q7zm56ZqduFV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404489403,"user_tz":-180,"elapsed":593,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["# разные режимы датасета \n","DATA_MODES = ['train', 'val', 'test']\n","# все изображения будут масштабированы к размеру 224x224 px\n","#RESCALE_SIZE = 224\n","# работаем на видеокарте\n","DEVICE = torch.device(\"cuda\")"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XO8XjFQdfXu4","colab_type":"text"},"source":["https://jhui.github.io/2018/02/09/PyTorch-Data-loading-preprocess_torchvision/\n","\n","Ниже мы исспользуем враппер над датасетом для удобной работы. Вам стоит понимать, что происходит с LabelEncoder и с torch.Transformation.\n","\n","ToTensor конвертирует PIL Image с параметрами в диапазоне [0, 255] (как все пиксели) в FloatTensor размера (C x H x W) [0,1] , затем производится масштабирование: $input = \\frac{input - \\mu}{\\text{standard deviation}} \\$, константы - средние и дисперсии по каналам на основе ImageNet\n","\n","Стоит также отметить, что мы переопределяем метод getitem для удобства работы с данной структурой данных. Также используется LabelEncoder для преобразования строковых меток классов в id и обратно. В описании датасета указано, что картинки разного размера, так как брались напрямую с видео, поэтому следуем привести их к одному размер (это делает метод _prepare_sample)"]},{"cell_type":"code","metadata":{"id":"DgGiDjogduAS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404490246,"user_tz":-180,"elapsed":560,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["class SimpsonsDataset_new(Dataset):\n","    \"\"\"\n","    Датасет с картинками, который паралельно подгружает их из папок\n","    производит скалирование и превращение в торчевые тензоры\n","    \"\"\"\n","    def __init__(self, files, mode):\n","        super().__init__()\n","        # список файлов для загрузки\n","        self.files = sorted(files)\n","        # режим работы\n","        self.mode = mode\n","\n","        if self.mode not in DATA_MODES:\n","            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n","            raise NameError\n","\n","        self.len_ = len(self.files) \n","        self.label_encoder = LabelEncoder()\n","\n","        if self.mode != 'test':\n","            self.labels = [path.parent.name for path in self.files]\n","            self.label_encoder.fit(self.labels)\n","            # Кодируем наши классы числами, и сохраняем в файл\n","            # print(dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_))))\n","            with open('label_encoder.pkl', 'wb') as le_dump_file:\n","                  pickle.dump(self.label_encoder, le_dump_file)\n","    # переопределяем метод len как количество файлов                  \n","    def __len__(self):\n","        return self.len_\n","    # подгружаем и возвращает картинку  \n","    def load_sample(self, file):\n","        image = Image.open(file)\n","        image.load()\n","        return image\n","  \n","    def __getitem__(self, index):\n","        # для преобразования изображений в тензоры PyTorch и нормализации входа\n","        if self.mode == 'train':\n","            transform = transforms.Compose([\n","              #transforms.RandomRotation(25),\n","              transforms.RandomHorizontalFlip(),\n","              transforms.Resize((299,299)),\n","              transforms.ToTensor(),\n","              transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n","        ])\n","        if self.mode == 'vale' or 'test':\n","            transform = transforms.Compose([\n","              transforms.Resize((299,299)),\n","              transforms.ToTensor(),\n","              transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n","        ])\n","        # подгружаем картинку\n","        x = self.load_sample(self.files[index])\n","        # ресайзим и возвращаем\n","        #x = self._prepare_sample(x) прям в композе засунем \n","        # нормализуем\n","        #x = np.array(x / 255, dtype='float32') transforms.ToTensor() итак переводит из диапазона [0, 255] в диапазон [0.0, 1.0]\n","        # трансформируем по функии выше\n","        x = transform(x)\n","        if self.mode == 'test':\n","            return x\n","        else:\n","            label = self.labels[index]\n","            label_id = self.label_encoder.transform([label])\n","            y = label_id.item()\n","            return x, y\n","    # ресайз    \n","    def _prepare_sample(self, image):\n","        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n","        return np.array(image)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIi_4x8rhoNU","colab_type":"text"},"source":["OverSampling (Самописный велосипед =)\n","\n","Он заключается в том, чтобы докопировать количество картинок из меньших классов,так, чтобы общее число картинок было таким же как и у классов с большим числом картинок.\n","\n","Если мы поступим таким образом наш dataset разрастется до таких размеров, что перестанет влазить в оперативную память видеокарты.\n","\n","Можно однако сделать по другому, глядя на полученные нами таблицы точности мы видим, что без балансировки все классы, где было не менее 100 картинок, отлично распознаются.\n","\n","Т.е. мы просто откопируем картинки малочисленных классов менее 100 картинок из выборки train, так чтобы число на каждый малочисленный класс было не менее 100.\n","\n","Если мы сделаем oversampling, - возьмем картинки из всей выборки до разбиения ее на 'train' и 'val' то скорее всего в train попадут и картинки из выборки val, а мы этого не хотим.\n","\n","При этом у нас все так же будет работать аугментация, да картинки в малочисленных классах будут одинаковыми, но при каждой загрузке одни и те же картинки будут по разному аугментироваться, и будут отличаться друг от друга. Конечно такого разнообразия как у изначально многочисленных классов не будет, но это лучше чем ничего.\n","\n","Напишем такую функцию вручную. Нам не надо копировать картинки, нам надо копировать пути к картинкам !"]},{"cell_type":"code","metadata":{"id":"xKuE9sgQhlgj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404491459,"user_tz":-180,"elapsed":945,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["TRAIN_DIR = Path('train/simpsons_dataset')\n","TEST_DIR = Path('testset/testset')\n","\n","train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n","test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"gLMaFMathldC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404491460,"user_tz":-180,"elapsed":564,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["from sklearn.model_selection import train_test_split\n","train_val_labels = [path.parent.name for path in train_val_files] # классы train + val\n","train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n","                                          stratify=train_val_labels)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNP8I2MphuNd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404491793,"user_tz":-180,"elapsed":502,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["train_labels = [path.parent.name for path in train_files] # классы train\n","val_labels = [path.parent.name for path in val_files]     # классы val"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJUPdRUhhwT7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404492143,"user_tz":-180,"elapsed":482,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["def create_dct_path_labels(train_files, train_labels):\n","    dct_simpsons = {}\n","    for label_i in np.unique(train_labels).tolist():\n","        dct_simpsons[label_i] = []\n","\n","    for path_i, label_i in zip(train_files, train_labels):\n","        dct_simpsons[label_i].append(path_i)\n","\n","    return dct_simpsons"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Grn-LisVhwa0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404492544,"user_tz":-180,"elapsed":571,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["# Создадим словарь в котором ключами будут персонажи Симпсонов, а значениями списки с путями к картинкам.\n","dct_path_train = create_dct_path_labels(train_files, train_labels)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqJEOhk9hwXb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404492861,"user_tz":-180,"elapsed":432,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["#Получился у нас словарь в нем ключ это персонаж из Симпсонов, а значение это список содержащий пути к картинкам\n","#list = [PosixPath('dataset/train/simpsons_dataset/waylon_smithers/pic_0033.jpg'), PosixPath('dataset/train/simpsons_dataset/waylon_smithers/pic_0139.jpg').....]"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"nswo2pEth1Ls","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404493705,"user_tz":-180,"elapsed":652,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["# Дополним картинки классов у которых менее 100 картинок, до 100 картинок в классе\n","for person in dct_path_train:\n","    if len(dct_path_train[person]) < 100:\n","        dct_path_train[person] = dct_path_train[person] * (100 // len(dct_path_train[person]))\n","        dct_path_train[person].extend(dct_path_train[person][:100 - len(dct_path_train[person])])"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"74E2OrgPh1Ob","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":722},"executionInfo":{"status":"ok","timestamp":1597404493997,"user_tz":-180,"elapsed":439,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}},"outputId":"1546d217-86a8-4160-9bcc-ee1cc89a1e75"},"source":["for person in dct_path_train:\n","    print(f\"{person}\\t{len(dct_path_train[person])}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["abraham_grampa_simpson\t685\n","agnes_skinner\t100\n","apu_nahasapeemapetilon\t467\n","barney_gumble\t100\n","bart_simpson\t1006\n","carl_carlson\t100\n","charles_montgomery_burns\t895\n","chief_wiggum\t739\n","cletus_spuckler\t100\n","comic_book_guy\t352\n","disco_stu\t100\n","edna_krabappel\t343\n","fat_tony\t100\n","gil\t100\n","groundskeeper_willie\t100\n","homer_simpson\t1684\n","kent_brockman\t373\n","krusty_the_clown\t904\n","lenny_leonard\t233\n","lionel_hutz\t100\n","lisa_simpson\t1015\n","maggie_simpson\t100\n","marge_simpson\t968\n","martin_prince\t100\n","mayor_quimby\t185\n","milhouse_van_houten\t809\n","miss_hoover\t100\n","moe_szyslak\t1089\n","ned_flanders\t1090\n","nelson_muntz\t269\n","otto_mann\t100\n","patty_bouvier\t100\n","principal_skinner\t895\n","professor_john_frink\t100\n","rainier_wolfcastle\t100\n","ralph_wiggum\t100\n","selma_bouvier\t100\n","sideshow_bob\t658\n","sideshow_mel\t100\n","snake_jailbird\t100\n","troy_mcclure\t100\n","waylon_smithers\t136\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VH8eUMPkh1X4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404494946,"user_tz":-180,"elapsed":475,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["new_train_files = []\n","\n","for person in dct_path_train:\n","    new_train_files.extend(dct_path_train[person])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zo5LjhM2h1gl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404496055,"user_tz":-180,"elapsed":983,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["#Отлично, мы видим, что теперь все классы имеют минимум 100 картинок. У нас получилось.\n","\n","val_dataset = SimpsonsDataset_new(val_files, mode='val')\n","new_train_dataset = SimpsonsDataset_new(new_train_files, mode='train')"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o4oxvwizh_oz","colab_type":"text"},"source":["## **FIT**"]},{"cell_type":"code","metadata":{"id":"kVWbkWXpjzCS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404496791,"user_tz":-180,"elapsed":751,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["inception_model = models.inception_v3(pretrained=True,aux_logits=False)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oLZUwDOCj6Dw","colab_type":"text"},"source":["Finally, notice that inception_v3 requires the input size to be (299,299), whereas all of the other models expect (224,224)."]},{"cell_type":"code","metadata":{"id":"EWKVThQwh1eL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404497512,"user_tz":-180,"elapsed":457,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["def fit_epoch(model, train_loader, criterion, optimizer):\n","    running_loss = 0.0\n","    running_corrects = 0\n","    processed_data = 0\n","  \n","    for inputs, labels in train_loader:\n","       # батчи, 3цвета, размер картинки torch.Size([128, 3, 299, 299]) / батча 128 должно хватить(не всю сеть переобучать буду)\n","        inputs = inputs.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs) # инспетион/ 128 картинок с вероятностью для каждого класса torch.Size([128, 42])\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        preds = torch.argmax(outputs, 1) # по каждой картинке макс. вероятность - класс\n","        running_loss += loss.item() * inputs.size(0) # умножамем лосс (энтропию) на рамзер батча\n","        running_corrects += torch.sum(preds == labels.data)\n","        processed_data += inputs.size(0)\n","              \n","    train_loss = running_loss / processed_data # средний лос по картинке\n","    train_acc = running_corrects.cpu().numpy() / processed_data # перевести в cpu (в куде не считает )  - процент прав. ответов\n","    return train_loss, train_acc"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvp8ntgWh1bx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404498327,"user_tz":-180,"elapsed":537,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["def eval_epoch(model, val_loader, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    processed_size = 0\n","\n","    for inputs, labels in val_loader:\n","        inputs = inputs.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        with torch.set_grad_enabled(False):\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            preds = torch.argmax(outputs, 1)\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","        processed_size += inputs.size(0)\n","    val_loss = running_loss / processed_size\n","    val_acc = running_corrects.double() / processed_size # is equivalent to self.to(torch.float64)\n","    return val_loss, val_acc"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUdtsRkbh1Uz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404564921,"user_tz":-180,"elapsed":690,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["def train(train_files, val_files, model, epochs, batch_size):\n","    train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    history = []\n","    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n","    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n","\n","    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n","        opt = torch.optim.AdamW(param_for_train,lr = 0.0001)\n","        criterion = nn.CrossEntropyLoss()\n","        scheduler=torch.optim.lr_scheduler.StepLR(opt,7,gamma=0.1)\n","\n","        for epoch in range(epochs):\n","            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n","            print(\"loss\", train_loss)\n","            \n","            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n","            history.append((train_loss, train_acc, val_loss, val_acc))\n","            scheduler.step()\n","            # обновляем прогресс бар\n","            pbar_outer.update(1)\n","            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n","                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n","            \n","    return history"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVGzKCFijGz9","colab_type":"text"},"source":["про файн тюнинг https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4\n","\n","https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"]},{"cell_type":"code","metadata":{"id":"H5PwpLh6h1SJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597404505719,"user_tz":-180,"elapsed":4690,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["## Freezing the first few layers. Here I am freezing the first 7 layers \n","ct = 0\n","for name, child in inception_model.named_children():\n","    ct += 1\n","    if ct < 7:\n","        for name2, params in child.named_parameters():\n","          params.requires_grad = False\n","\n","param_for_train = [i for i in inception_model.parameters() if i.requires_grad == True]\n","\n","num_features2 = 2048\n","inception_model.fc = nn.Linear(num_features2, 42)\n","inception_model = inception_model.to(DEVICE)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGvXOajzjEZa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":538},"executionInfo":{"status":"ok","timestamp":1597407841488,"user_tz":-180,"elapsed":3273162,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}},"outputId":"0636984d-ba10-4468-9ab4-91680265622e"},"source":["history = train(new_train_dataset, val_dataset, model=inception_model, epochs=10, batch_size=128)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["\repoch:   0%|          | 0/10 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 1.5197161421457597\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  10%|█         | 1/10 [05:32<49:55, 332.86s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 001 train_loss: 1.5197     val_loss 0.3749 train_acc 0.6890 val_acc 0.9278\n","loss 0.2350190382295369\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  20%|██        | 2/10 [10:58<44:05, 330.72s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 002 train_loss: 0.2350     val_loss 0.1524 train_acc 0.9407 val_acc 0.9612\n","loss 0.08738298260070192\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  30%|███       | 3/10 [16:25<38:26, 329.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 003 train_loss: 0.0874     val_loss 0.1878 train_acc 0.9783 val_acc 0.9547\n","loss 0.050104734844980986\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  40%|████      | 4/10 [21:51<32:50, 328.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 004 train_loss: 0.0501     val_loss 0.1353 train_acc 0.9865 val_acc 0.9683\n","loss 0.021001713947299563\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  50%|█████     | 5/10 [27:17<27:18, 327.68s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 005 train_loss: 0.0210     val_loss 0.1425 train_acc 0.9947 val_acc 0.9696\n","loss 0.023208634411906764\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  60%|██████    | 6/10 [32:45<21:51, 327.80s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 006 train_loss: 0.0232     val_loss 0.2237 train_acc 0.9929 val_acc 0.9475\n","loss 0.030518078359991055\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  70%|███████   | 7/10 [38:11<16:21, 327.28s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 007 train_loss: 0.0305     val_loss 0.1448 train_acc 0.9920 val_acc 0.9654\n","loss 0.006334151875447635\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  80%|████████  | 8/10 [43:39<10:55, 327.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 008 train_loss: 0.0063     val_loss 0.1200 train_acc 0.9983 val_acc 0.9757\n","loss 0.0019571092127405876\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:  90%|█████████ | 9/10 [49:06<05:27, 327.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 009 train_loss: 0.0020     val_loss 0.1233 train_acc 0.9994 val_acc 0.9763\n","loss 0.0014988329549918207\n"],"name":"stdout"},{"output_type":"stream","text":["epoch: 100%|██████████| 10/10 [54:32<00:00, 327.25s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 010 train_loss: 0.0015     val_loss 0.1262 train_acc 0.9995 val_acc 0.9765\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Bb16u-6bjEod","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597407848012,"user_tz":-180,"elapsed":651,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["def predict(model, test_loader):\n","    with torch.no_grad():\n","        logits = []\n","    \n","        for inputs in test_loader:\n","            inputs = inputs.to(DEVICE)\n","            model.eval() # исключает в оценке ненужные слои например Dropouts Layers (выпали чассть нейронов)\n","            outputs = model(inputs).cpu()\n","            logits.append(outputs)\n","            \n","    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy() #нужен софтмакс, тк. ентропия в лоссе а слоя нет\n","    return probs"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0a5XNFCjEkn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597407849579,"user_tz":-180,"elapsed":846,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOczPHmJjEhp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597407857650,"user_tz":-180,"elapsed":7570,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["test_dataset = SimpsonsDataset_new(test_files, mode=\"test\")\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=128)\n","probs = predict(inception_model, test_loader)\n","\n","preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n","test_filenames = [path.name for path in test_dataset.files]"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNCdP1aMjEds","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597407913375,"user_tz":-180,"elapsed":612,"user":{"displayName":"Итс Ми","photoUrl":"","userId":"15686203201616133425"}}},"source":["import pandas as pd\n","df = pd.DataFrame()\n","df['Id'] = test_filenames\n","df['Expected'] = preds\n","df.to_csv('gdrive/My Drive/simpsons_simple_cnn_baseline_new.csv', index=False)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HtZha-JlZmNv","colab_type":"text"},"source":[""]}]}